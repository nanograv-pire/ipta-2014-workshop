{
 "metadata": {
  "name": "",
  "signature": "sha256:5990d76da62688b533b011ba4b47ebb2519e690038295230027d7f2145612e52"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h1>Data analysis with sinusoid of unknown amplitude in known Gaussian noise</h1>\n",
      "\n",
      "<p>\n",
      "    The problem you are faced with is detecting and characterizing a sinusoid\n",
      "    in Gaussian noise.  This is a standard \n",
      "    and simple problem in data analysis that will give you a flavor of the data\n",
      "    analysis problems and techniques used in real PTA data analysis. In fact, \n",
      "    nearly the entire GW detection problem is framed in this way: Is there a\n",
      "    detectable GW (be it stochastic, continuous, or burst) present in the puslar\n",
      "    timing residual data? The challenge is coming up with robust and efficient \n",
      "    analysis methods that take into account all of the intracacies of real puslar\n",
      "    timing data.\n",
      "    <br/><br/>\n",
      "    Dealing with real or simulated PTA data will only cause additional complications\n",
      "    and will deter you from learning the basics of data analysis. In this activity \n",
      "    you will explore both frequentist and Bayesian techniques for parameter estimation, \n",
      "    detection, and setting upper limits. <br/><br/>\n",
      "    We assume that the data $d$ is composed of a signal $s$ plus additive white gaussian \n",
      "    noise $n$, that is\n",
      "    $$\n",
      "    d=s+n.\n",
      "    $$\n",
      "    Since $n$ is assumed to be white and Gaussian, its probability distribution is a product of\n",
      "    Gaussians\n",
      "    $$\n",
      "    p(n)=\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\prod_i\\exp\\left(-\\frac{n_i^2}{2\\sigma^2}\\right),\n",
      "    $$\n",
      "    where $n_i$ is the noise at time $t_i$ and $\\sigma$ is the standard deviation of the noise. \n",
      "    We can now write this in terms of the data using $n=d-s$, therefore the likelihood function\n",
      "    for the data given the signal parameters, $\\lambda$ is\n",
      "    $$\n",
      "    p(d|\\lambda)=\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\prod_i\\exp\\left(-\\frac{(d_i-s_i(\\lambda))^2}{2\\sigma^2}\\right).\n",
      "    $$\n",
      "    For our purposes we will deal with the natural log of the likelihood ratio. The likelihood ratio is the \n",
      "    likelihood function above divided by the likelihood that the signal is pure noise (i.e, remove\n",
      "    the signal $s$ from the model). We will also introduce a new notation and write the log-likelihood \n",
      "    ratio\n",
      "    $$\n",
      "    \\log\\Lambda=(d|s)-\\frac{1}{2}(s|s),\n",
      "    $$\n",
      "    where the inner product of two timeseries $x$ and $y$, for example, is $(x|y)=\\sum_i x_iy_i/\\sigma^2$.\n",
      "    In our case the signal $s$ is just a sinusoid, $s(t,A,f) = A\\sin(2\\pi ft)$. For simplicity we will \n",
      "    assume that we know the frequency (or that we are doing a \"target\" search for a certain frequency) \n",
      "    of the sinusoid and the only unknown parameter is the amplitude.\n",
      "    <br/><br/>\n",
      "    Your first exercise will be to create a simulated dataset with white noise and a small sinusoid. \n",
      "    <br/><br/>\n",
      "    Good luck!\n",
      "\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import worksheet_utils as wu\n",
      "from scipy.interpolate import interp1d"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h2>0. Derving the log-likelihod ratio</h2>\n",
      "\n",
      "<p>\n",
      "    Above we have written down the log of the likehood ratio which will be used throughout this \n",
      "    entire worksheet. To get yourself familiar with the notation, derive the log-likelihood ratio\n",
      "    defined above. \n",
      "    <br/><br/>\n",
      "    <em>Hint</em>: It will be easier to take the logarithm of each likelihood function first and\n",
      "    then take the difference (i.e., $\\log(A/B)-\\log(A)-\\log(B)$).\n",
      "</p>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h2>1. Simulating the data set. </h2>\n",
      "\n",
      "<p>\n",
      "    Create a 5 year timeseries of 130 points using `numpy` function <strong>`linspace()`</strong> beginning at 0. \n",
      "    Then, use the <strong>`simData()`</strong> function from `worksheet_utils` to simulate some fake data with a standard devitaion of \n",
      "    100 ns and a signal with amplitude 70 ns and frequency of $10^{-8}$ Hz.\n",
      "    <br/><br/>\n",
      "    Then use the `matplotlib` function __`plot()`__ to plot the data vs. time and be sure to label the axes. Next, create a separate dataset with no noise by calling the __`signal()`__ function from `worksheet_utils` in order to plot just the signal on top of the signal+noise dataset.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# define frequency, Amplitude, timeseries and sigma\n",
      "\n",
      "# call function to simulate data\n",
      "\n",
      "# call wu.signal function to simulate sine wave individually\n",
      "\n",
      "# plot data in blue with signal in red"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h2>2. Parameter Estimation: Estimating the Amplitude</h2>\n",
      "\n",
      "<p>\n",
      "    In frequentist statistics, one is interested in defining some estimator for the true signal parameters.\n",
      "    A common way to do this is to find the parameters that maximize the likelihood. These are known as Maximum\n",
      "    Likelihood Estimators (MLEs). One can then place confidence intervals on the parameter of interest through the \n",
      "    <em>Neyman method</em>: define some statistic $x$ that is a function of the data $d$ (for example the MLE of the\n",
      "    unknown signal parameters). Because $x$ is derived from $d$, the probability distribution (likelihood) $p(d|\\lambda)$\n",
      "    can be re-expressed as a probability distribution $p(x|\\lambda)$. Then, for each value of $\\lambda$, we produce an \n",
      "    interval $(x_1,x_2)$ such that\n",
      "    $$\n",
      "    \\alpha = \\int_{x_1}^{x_2}dx\\,\\, p(x|\\lambda)\n",
      "    $$\n",
      "    The intervals that depend on $\\lambda$ define a belt in the $\\lambda-x$ plane and for any <em>observed</em> value\n",
      "    of $x$ in the experiment, the confidence interval on $\\lambda$ constitutes the values of $\\lambda$ that exist on the \n",
      "    belt at that fixed value of $x$. (See additional handout for more details)\n",
      "    <br/><br/>\n",
      "    In Bayesian statistics, one is interested finding the probability distribution function for the unknown parameters given\n",
      "    that we have some observed data. To get a point estimate of the unkwnown parameters we may use the <em>maximum a-posteriori</em>\n",
      "    (MAP) parameters, that is, the maximum of the posterior probability distribution. A credible interval (Bayesian version\n",
      "    of confidence intervals) can be constructed directly from the posterior distribution $p(\\lambda|d)$ as\n",
      "    $$\n",
      "    \\alpha = \\int_{\\lambda_1}^{\\lambda_2}d\\lambda\\,\\, p(\\lambda|d),\n",
      "    $$\n",
      "    where $\\lambda_1$ and $\\lambda_2$ defined the lower and upper bounds on our parameter $\\lambda$.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>2.1 MLE for the amplitude</h3>\n",
      "\n",
      "<p>\n",
      "    As written above, the log-likelihood ratio is \n",
      "    $$\n",
      "    \\log\\Lambda=(d|s)-\\frac{1}{2}(s|s).\n",
      "    $$\n",
      "    If we let $s(t)=A\\sin(2\\pi f t)$ and $\\tilde{s}(t)=\\sin(2\\pi f t)$, then $s(t)=A\\tilde{s}(t)$. \n",
      "    Re-writing the log-likehood ratio, we obtain\n",
      "    $$\n",
      "    \\log\\Lambda=A(d|\\tilde{s})-\\frac{A^2}{2}(\\tilde{s}|\\tilde{s}).\n",
      "    $$\n",
      "    We can now maximize the likehood function over the unknown amplitude, $A$, as follows\n",
      "    $$\n",
      "    0 = \\frac{\\partial \\log\\Lambda}{\\partial A} = (d|\\tilde{s}) -\\hat{A}(\\tilde{s}|\\tilde{s}),\n",
      "    $$\n",
      "    where $\\hat{A}$ is the MLE for $A$. Solving for $\\hat{A}$, we obtain\n",
      "    $$\n",
      "    \\hat{A}=\\frac{(d|\\tilde{s})}{(\\tilde{s}|\\tilde{s})}.\n",
      "    $$\n",
      "    The variance on the maximum likelihood estimator is then\n",
      "    $$\n",
      "    \\sigma_{\\hat{A}}^2 = \\langle \\hat{A}\\hat{A} \\rangle -\\langle \\hat{A} \\rangle^2 = \\frac{1}{(\\tilde{s}|\\tilde{s})},\n",
      "    $$\n",
      "    where $\\langle  \\rangle$ denotes the <em>expectation value</em> or average over many realizations of data.\n",
      "<ol>\n",
      "    <li>\n",
      "        Write a function that reads in the data, the time samples, the frequency of the sine wave and the \n",
      "        standard deviation of the noise as arguments and outputs the MLE and standard deviation of $A$. \n",
      "        <em>Hint:</em> Look at the `worksheet_utils` package to figure out what functions you may need.<br/>\n",
      "        <span style=\"color:red\">For intermediate/expert level: Derive the variance of $\\hat{A}$. <em>Hint:</em> \n",
      "        $\\langle (n|\\tilde{s}) \\rangle=0$ and $\\langle (n|\\tilde{s})(n|\\tilde{s}) \\rangle=(\\tilde{s}|\\tilde{s})$.</span>\n",
      "    </li>\n",
      "    <li>\n",
      "        Use this function to compute $\\hat{A}$ for our simulated data set.\n",
      "    </li>   \n",
      "    <li>Does the answer you get make sense? What is the percent error?</li>\n",
      "</ol>\n",
      "</p>   "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#  maximum likelihood estimator for A. fill in function\n",
      "def maxLikeA(data, t, f, sigma):\n",
      "    return "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# call maximum likelihood function and print MLE along with the true value of A"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>2.2 Posterior distribution for the amplitude</h3>\n",
      "\n",
      "<ol>\n",
      "    <li>\n",
      "        Write a function that reads in the data, the time samples, the amplitude of the sine wave, the frequency of the sine wave\n",
      "        and the standard deviation of the noise and returns the log-likelihood ratio.\n",
      "    </li>\n",
      "    <li>\n",
      "        Create a vector of trial amplitudes in the range $[0, 5\\times 10^{-7}]$ with length 1000 using\n",
      "        the __`np.linspace()`__ function. You can play around with the length of the array and the ranges\n",
      "        once you have done this once.\n",
      "    </li>    \n",
      "    <li>\n",
      "        Loop over these amplitudes and compute the log-posterior for each one and store\n",
      "        it in an array. Here we will use a uniform prior on $A$ which means that the likelihood\n",
      "        and the posterior are identical except for a normalizing constant which is not important \n",
      "        here. The posterior is then\n",
      "        $$\n",
      "        \\log\\,p(A|d) \\propto \\log\\Lambda(d|A) + \\log p(A),\n",
      "        $$\n",
      "        where p(A) is the prior. \n",
      "    </li>\n",
      "    <li>\n",
      "        This prior must be normalized to 1, so if our prior is uniform in $A$ then\n",
      "        it is the save for every value of $A$, thus $p(A)=C$, where $C$ is a constant. \n",
      "        Integrate this uniform prior to determine\n",
      "        the normalization constant (i.e., use $\\int p(A)dA=1$ to find the value of $C$).\n",
      "    </li>   \n",
      "    <li>\n",
      "        Use the __`plt.plot()`__ function to plot the posterior vs. the \n",
      "        Amplitude Array.\n",
      "    </li>    \n",
      "    <li>\n",
      "        Determine the MAP value of $A$ by using the __`np.argmax()`__ function to find the array index\n",
      "        of the maximum posterior value. You can then use this array index to find the corresponding\n",
      "        value of $A$ from the vector of trial amplitudes that you created in step 2.\n",
      "    </li>    \n",
      "    <li>\n",
      "        Determine the standard deviation of $A$. <em>Hint:</em> recall that the statistical definition\n",
      "        of variance on a parameter $x$ whose pdf is $p(x)$ is $\\sigma^2_x=\\langle x^2 \\rangle - \\langle x \\rangle^2 \n",
      "        = \\int dx \\, x^2\\,p(x) - \\left(\\int dx\\, x\\, p(x)\\right)^2$\n",
      "    </li>    \n",
      "    <li>\n",
      "        Plot the likelihood vs $A$. Plot a vertical line denoting the true value of $A$ with the __`plt.axvline()`__ \n",
      "        function. Also, use the __`plt.axvspan()`__ with the <em>alpha</em> argument to plot a shaded region denoting\n",
      "        the standard deviation of $A$.\n",
      "    </li>\n",
      "</ol>\n",
      "    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# log-likelihood ratio function (fill in function)\n",
      "def LogLikelihood(data, t, A, f, sigma):\n",
      "    return "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create amplitude vector and initialize log likelihood vector\n",
      "\n",
      "# loop over amplitude value and call log likelihood function for each amplitude\n",
      "\n",
      "# exponentiate log-likelihood and multiply by prior to get the posterior distribution\n",
      "\n",
      "# use argmax function to find index of maximum value of the posterior. Use array index to find MAP value of A\n",
      "\n",
      "# print MAP value of A and true value\n",
      "\n",
      "# plot posterior vs A in blue and vertical line at the injected value in red\n",
      "\n",
      "# use axhspan to make shaded region denoting 1 standard deviation\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h4>2.2.1: Gaussian prior distribution on $A$</h4>\n",
      "\n",
      "<p>\n",
      "    It is often the case in data analysis where we have some additional prior information about our problem before\n",
      "    conducting the experiment. In the Bayesian framework we incorporate this additional information into the prior\n",
      "    probability distribution. The data (via the likelihood function) is then used to update our prior knowledge. If\n",
      "    our data is <em>informative</em> then the posterior distribution will be different then the prior distribution. \n",
      "    On the other hand, if our posterior results in distribution that is identical to the prior then our data is not\n",
      "    informative.\n",
      "    <br/><br/>\n",
      "    To illustrate this, we will assume that through some other experiment, or from some theoretical prediction, we have\n",
      "    some prior knowledge of the amplitude of the sine wave in our data. We will model this as a gaussian prior with the\n",
      "    mean the true value that we used to create the data and some standard deviation\n",
      "    $$\n",
      "    p(A)=\\frac{1}{\\sqrt{2\\pi\\sigma_A^2}}\\exp\\left(-\\frac{(A-A_{\\rm true})^2}{2\\sigma_A^2}\\right)\n",
      "    $$\n",
      "</p>\n",
      "\n",
      "<ol>\n",
      "    <li>\n",
      "        Let the fractional uncertainty on $A$ be 25%, that is $\\sigma_A=0.25\\times A$. Use this information in the\n",
      "        gaussian prior and repeat the steps from exercise 2.2. You will need to create an extra array to store the \n",
      "        prior function when conducting the for loop. When constructing the posterior, remember to  multiply the \n",
      "        likelihood by the prior.\n",
      "    </li>\n",
      "    <li>\n",
      "        Make sure that the posterior and prior both are both normalized and plot the posterior and the prior vs. A.\n",
      "        Is the data informative?\n",
      "    </li>\n",
      "    <li>\n",
      "        Repeat this exercise for $\\sigma_A=0.05\\times A$.\n",
      "    </li>\n",
      "</ol>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# repeat above exercise with the gaussian prior shown above"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>2.3 Frequentist Confidence Interval on Amplitude Parameter</h3>\n",
      "\n",
      "<p>\n",
      "    As we show above, contructing a frequentist confidence interval involves simulating several realizations of data with different values of the sinusoid amplitude. Here we will construct a function to do just that.\n",
      "</p>\n",
      "\n",
      "<ol>\n",
      "    <li>\n",
      "        Create a vector of trial amplitudes in the range $[1\\times 10^{-8}, 3\\times 10^{-7}]$ with length 100 using\n",
      "        the __`linspace()`__ function.\n",
      "    </li>    \n",
      "    <li>\n",
      "        Loop over these amplitudes as above but now add a second inner loop to simulate different <em>\n",
      "        realizations</em> of the data. Use __`simData()`__ function to simulate the new data. For each\n",
      "        amplitude do 1000 realizations of data with that amplitude. (See code block below for hints)\n",
      "    </li>    \n",
      "    <li>\n",
      "        The goal here is to compute the MLE for $A$ for each realization of data. For a given amplitude,\n",
      "        compute $\\hat{A}$ for each data realization using your function from exercise 2.1 and store it in an\n",
      "        array. Then use the __`confinterval()`__ function from `worksheet_utils` to define the upper and lower 1-sigma bounds on the\n",
      "        distribution of $\\hat{A}$ for a given value of $A$. Store these 1-sigma lower and upper\n",
      "        bounds in arrays. This will create the band through the $A$-$\\hat{A}$ space as we saw in the\n",
      "        talk. (See code block below for hints)\n",
      "    </li>    \n",
      "    <li>\n",
      "        After you have looped over all values of the amplitude. Use the <a href=\"http://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html\">__`interp1d()`__</a> function to\n",
      "        interpolate the 1-sigma upper and lower bounds on $\\hat{A}$ for each value of $A$.\n",
      "        This way we create an interpolating function so that we can evaluate the\n",
      "        confidence interval on $A$ from our confidence belt and our measurement of $\\hat{A}$. \n",
      "    </li> \n",
      "    <li>\n",
      "        Plot the confidence band (i.e., the injected amplitude vs the 1-sigma upper and lower bounds on $\\hat{A}$.) as\n",
      "        well as a vertical line denoting the measured value of $\\hat{A}$ and a shaded area using the __`plt.axhspan()`__\n",
      "        function to denote the confidence interval on $A$.\n",
      "    </li>\n",
      "</ol>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# set number of amplitudes and create amplitude vector\n",
      "\n",
      "# set number of realizations and intialize arrays for the 1-sigma bounds on Ahat\n",
      "\n",
      "# loop over amplitudes\n",
      "\n",
      "# for ii in range(N_amplitudes):\n",
      "\n",
      "    # loop over realizations\n",
      "    # for jj in range(N_realizations):\n",
      "    \n",
      "        # simulate data set and get Ahat for new dataset\n",
      "        \n",
      "    # get 1-sigma confidence intervals on Ahat\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# interpolate to get inverse function for the injected amplitude\n",
      "# as a function of the confidence regions on Ahat. We want two\n",
      "# interpolating functions, one for the upper bounds on Ahat and\n",
      "# one for the lower bounds on Ahat. The x values of the interpolating\n",
      "# function should be the lower/upper bounds on Ahat for each value of A,\n",
      "# and the y values should be the values of A themselves\n",
      "\n",
      "\n",
      "# use the interpolation functions and evaluate them at the measured value of Ahat for our original dataset\n",
      "\n",
      "# print the corresponding confidence region on A\n",
      "\n",
      "# plot the confidence belt, measured value of Ahat and a shaded confidence interval"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3> 2.4 Bayesian Credible Interval on Amplitude Parameter</h3>\n",
      "\n",
      "<p>\n",
      "    As was discussed in the talk and have shown above, Bayesian credible intervals use\n",
      "    the likelihood function directly to measure our confidence, or degree of belief\n",
      "    in our measurement of the Amplitude parameter. In practice, this is much less \n",
      "    complicated than constructing frequentist confidence intervals. However in\n",
      "    more complex data analysis problems with many unknown parameters, just constructing\n",
      "    the full likelihood function is very difficult or at least computatinally demanding.\n",
      "</p>\n",
      "<ol>\n",
      "    <li>\n",
      "        Repeat steps 1-3 from exercise 2.2 to construct the likelihood function for $A$.\n",
      "    </li>    \n",
      "    <li>\n",
      "        Write down an algorithm for computing the Bayesian credible region. You do not have\n",
      "        to write actual code here unless you feel comfortable. Simply think about how you \n",
      "        would set up the problem. <em>Hint:</em> Remember, the desired integral is        \n",
      "        $$\n",
      "        \\alpha = \\int_{\\lambda_1}^{\\lambda_2}d\\lambda\\,\\, p(\\lambda|d)\n",
      "        $$\n",
      "        but it can be re-written as        \n",
      "        $$\n",
      "        \\frac{1-\\alpha}{2} = \\int_{-\\infty}^{\\lambda_1}d\\lambda\\,\\, p(\\lambda|d)\n",
      "        $$        \n",
      "        and        \n",
      "        $$\n",
      "        \\frac{1+\\alpha}{2} = \\int^{\\infty}_{\\lambda_2}d\\lambda\\,\\, p(\\lambda|d)\n",
      "        $$\n",
      "    </li>    \n",
      "    <li>\n",
      "        Use the pre-made function __`confinterval_like()`__ to construct the 1-sigma upper\n",
      "        and lower bounds on $A$.\n",
      "    </li>    \n",
      "    <li>\n",
      "        Plot the likelihood function vs. $A$ along with the 1-sigma upper and lower bounds, \n",
      "        again using the __`plt.plot()`__ and __`plt.axvline()`__ functions.\n",
      "    </li>\n",
      "</ol>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# repeat steps 1-3 of exercise 2.2\n",
      "\n",
      "# exponentiate likelihood function and compute confidence interval\n",
      "\n",
      "# print upper and lower bounds of credible region\n",
      "\n",
      "# plot the likelihood vs. A in blue along with vertical lines for the upper/lower bounds"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h2>3. Hypothesis Testing: Detecting the Signal</h2>\n",
      "\n",
      "<p>\n",
      "    Above, when performing parameter estimation we have taken for granted that there was an <em>detectable</em>\n",
      "    signal in the data. Now we turn to the problem of detection, that is, how can we confidently say that there\n",
      "    truly is a signal in the data. In the frequentist framework, detection significance is usually based on the\n",
      "    false alarm probability (sometimes referred to as FAP, I know hilarious right?), that is, the probability \n",
      "    that we would (falsely) claim a detection when that data consists of only noise. Henceforth, we refer to this\n",
      "    noise only case as the <em>null hypothesis</em> and we denote it symbolically as $H_0$. On the other hand we\n",
      "    refer to the case where the data does contain a signal as the <em>signal hypythesis</em> and we denote it symbolically\n",
      "    as $H_1$. In the Bayesian framework, we actually compute the <em>evidence</em> for the signal and null hypotheses and\n",
      "    compare them via the Bayes factor. \n",
      "</p>\n",
      "\n",
      "<p>\n",
      "    In this next section, we will apply these techniques to our simple problem of the sinusoid with unknown amplitude.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>3.1 Frequentist Hypothesis Testing</h3>\n",
      "\n",
      "<p>\n",
      "    The false alarm probability is defined as   \n",
      "    $$\n",
      "    {\\rm FAP} = \\int_{\\hat x}^{\\infty} dx\\,\\, p(x|\\lambda,H_0),\n",
      "    $$    \n",
      "    where $\\hat{x}$ is our measured test statistic, $\\lambda$ is the \n",
      "    parameter of interest, and $p(x|\\lambda,H_0)$ is the pdf of $x$ under the null \n",
      "    hypothesis. The dependence on the actual data, comes in the value of $\\hat{x}$. Essentially what we are doing\n",
      "    is we are testing whether our point extimate of $x$ is consistent with a value of $x$ that we would measure if\n",
      "    the data contains only noise. So to claim a detection, we want to reject the null hypothesis, that is, we want \n",
      "    $\\hat{x}$ to be <em>inconsistent</em> with the distribution of $x$ under the null hypothesis and the false alarm\n",
      "    probability quantifies that inconsistency.\n",
      "</p>\n",
      "\n",
      "<p>\n",
      "    The value of the false alarm probability that one requires for detection is completely problem dependent. In some\n",
      "    cases we would be ok with a value of 5% (i.e., we make a false detection 5% of the time). For example, many of the\n",
      "    social sciences use this value. However, for our purposes we want to be much more confident so we will require \n",
      "    $FAP < 10^{-4}$ for a detection, that is, there is a 1 in 10,000 chance that we have made a false detection. It is\n",
      "    very important to note here that just because we can rule out the null hypothesis with $1-FAP$ confidence, does \n",
      "    <em>not</em> mean that the signal hypothesis is true with $1-FAP$ confidence. In fact, frequentist detection methods \n",
      "    make no statetments about our confidence in the signal model itself, only that a measurement made under the signal \n",
      "    hypothesis is inconsistent with the null hypothesis at the $1-FAP$ level.\n",
      "</p>\n",
      "\n",
      "<p>\n",
      "    Now we want to set the value of $\\hat{x}$ that would be required for us to have $FAP=10^{-4}$. \n",
      "</p>\n",
      "\n",
      "<ol>    \n",
      "    <li>\n",
      "        First, we want to construct our detection statistic. The <a href=\"http://en.wikipedia.org/wiki/Neyman\u2013Pearson_lemma\">\n",
      "        Neyman-Pearson lemma</a> states that the optimal detection statistic is the likelihood ratio. Use our expression for\n",
      "        the log-likelihood ratio and $\\hat{A}$ from exercise 2.1 to construct the maximum likelihood ratio. <em>Hint:</em>\n",
      "        you should get        \n",
      "        $$\n",
      "        \\log \\Lambda_{\\rm max} = \\frac{1}{2}\\frac{(d|\\tilde s)^2}{(\\tilde s|\\tilde s)}\n",
      "        $$\n",
      "    </li>    \n",
      "    <li>\n",
      "       As we did for the likelihood ratio. Write a function that reads in the data, the time samples, the frequency \n",
      "       of the sine wave and the standard deviation of the noise and returns the maximized log-likelihood ratio.\n",
      "    </li>    \n",
      "    <li>\n",
      "        Now we want to construct the pdf of the maximum log-likelihood ratio under the null hypothesis. As we did\n",
      "        in exercise 2.3, we want to loop over many realizations of data and compute the max log-likelihood ratio,\n",
      "        only this time we simulate only noise. Simulate 100000 realizations of data and store the maximum log-likelihood\n",
      "        values in an array and histogram the results with 50 bins using the __`plt.hist()`__ function.\n",
      "    </li>    \n",
      "    <li>\n",
      "        Now that we have the distribution $p(x|\\lambda,H_0)$, we want to determine the value of $\\hat{x}=\\hat{x}_{\\rm thresh}$\n",
      "        that will give a false alarm probability of $10^{-4}$. We have already build all of the machinery we require to determine\n",
      "        $\\hat{x}_{\\rm thresh}$. <em>Hint:</em> If we re-write the FAP integral, we can use the __`confinterval()`__ function\n",
      "        with the argument onesided=True. Please ask for help if you are stuck here.\n",
      "        <br/>\n",
      "        <span style='color:red'>Expert level: This distribution can be computed analytically. Do you know what it is?</span>\n",
      "    </li>    \n",
      "    <li>\n",
      "        Use our original data with a signal to determine if we have made a detection.\n",
      "    </li>    \n",
      "</ol>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# maximum log likelihood function. Fill in function\n",
      "def maxLogLikelihood(data, t, f, sigma):\n",
      "    return "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# number of realizations and initialize max log-likelihood array\n",
      "\n",
      "# loop over realizations\n",
      "        \n",
      "    # simulate data and calculate maximum log-likelihood value"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# plot histogram of maximum log-likelihood values\n",
      "\n",
      "# compute threshold value on the maximum log-likelihood ratio\n",
      "\n",
      "# compute maximum log-likelihood ratio and determine if we have made a detection"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>3.2 Bayesian Hypothesis Testing</h3>\n",
      "\n",
      "<p>\n",
      "    When condisering Bayesian parameter estimation, we used Bayes theorem for a <em>single</em> hypothesis. We now use the \n",
      "    more general form of Bayes theorem:    \n",
      "    $$\n",
      "    p(\\lambda,H|d) = \\frac{p(d|\\lambda,H)p(\\lambda,H)}{p(d)},\n",
      "    $$\n",
      "    where $p(\\lambda,H|d)$ is the probability of the joint posterior of parameters and the hypotheses, $p(d|\\lambda,H)$ is the likelihood of the data given a set of parameters and hypotheses, $p(\\lambda,H)=p(\\lambda|H)p(H)$ is the prior probability on the parameters and hypotheses and $p(d)$ is the probability of the data itself, also known as the evidence. Here we are interested in the evidence:    \n",
      "    $$\n",
      "    p(d)=\\sum_i\\int d\\lambda_i\\,p(d|\\lambda_i,H_i)p(\\lambda_i|H_i)p(H_i)= \\sum_i p(d|H_i)p(H_i)\n",
      "    $$\n",
      "    which is the sum of the marginal likelihoods for different Hypotheses. In some cases, there may be many possible hypotheses, but it is\n",
      "    always possible to compare different ones. In our case we want to <em>directly</em> calculate the evidence for hypotheses, $H_1$ and $H_0$.\n",
      "    The bayes factor is defined as the ratio of two marginal likelihoods    \n",
      "    $$\n",
      "    B_{10} = \\frac{p(d|H_1)}{p(d|H_0)}.\n",
      "    $$    \n",
      "    In large parameter spaces, this is notoriously hard to calculate; however, in our case it is trivial since our null \n",
      "    hypothesis model has no parameters and our signal model only has 1 parameter we have    \n",
      "    $$\n",
      "    B_{10}=\\int dA\\,\\, \\Lambda(A|d)p(A)\n",
      "    $$    \n",
      "    where $\\Lambda(A|d)$ is just the likelihood ratio defined above and $p(A)$ is the prior on $A$, which we take to be constant. \n",
      "    For our calculation, we will need to include the prior even though it is a constant. You can analytically find the normalizing\n",
      "    constant by integrating $p(A)=C$ over the range of $A$ (which is $[0,5\\times10^{-7}]$ if using the same as exercise 2.2) and \n",
      "    setting it equal to 1. That is, we require    \n",
      "    $$\n",
      "    \\int dA\\,\\, p(A) = 1\n",
      "    $$\n",
      "</p>\n",
      "\n",
      "<ol>\n",
      "    <li>\n",
      "        Loop over 1000 amplitude values and compute the log likelihood just like in exercise 2.2. Once you have the log-likelihood ratio \n",
      "        at each amplitude value you can exponentiate it to obtain the likelihood itself.\n",
      "    </li>    \n",
      "    <li>\n",
      "        Use the likelihood values to compute the bayes factor. Remember we can just use a block integral        \n",
      "        $$\n",
      "        \\int dA\\,\\, \\Lambda(A|d) \\approx \\sum_i\\Lambda(A_i|d) \\Delta A.\n",
      "        $$        \n",
      "        The value of the Bayes factor that is normally considered decisive evidence for model $H_1$ over model $H_0$ is 100. This \n",
      "        values comes from the so-called <a href=\"http://en.wikipedia.org/wiki/Bayes_factor#Interpretation\">Jeffreys' scale</a>. For\n",
      "        our purposes here we will follow this scale but in real data analysis applications is is an open problem as to which value \n",
      "        one would truly consider decisive, say for claiming the detection of GWs. One could implement a hybrid frequentist-Bayesian\n",
      "        method where the Bayes factor is used as our test statistic and we can set corresponding FAPs and thresholds for detection.\n",
      "    </li>\n",
      "</ol>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# repeat exercise 2.2 steps 1-3\n",
      "\n",
      "\n",
      "# compute bayes factor with blick integral being sure to normalize prior\n",
      "\n",
      "# print Bayes factor. Is it bigger than 100?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h4>3.2.1 Occam Penalty</h4>\n",
      "\n",
      "<p>\n",
      "    In hypothesis testing we generally want to choose the simplest model if it provides a good fit to the data. In essence, \n",
      "    we need to weigh the \"goodnes of fit\" versus the simplicity of the model. For example, we could use a free parameter for \n",
      "    every data point, then our model would fit the data perfectly but this model would have hundreds of parameters, versus just\n",
      "    one for our sinusoid model. The bayes factor automatically incorporates this parsimony through the integration of the posterior,\n",
      "    which contains our prior distribution. \n",
      "</p>\n",
      "\n",
      "<ol>\n",
      "    <li>\n",
      "        We can see this automatic parsimony by choosing a larger prior range on $A$ in the above calculation of the Bayes factor. \n",
      "        Repeat the calculation but now create the amplitude vector going up to $5\\times 10^{-6}$. <em>Hint:</em> Remember to \n",
      "        recalculate the normalization of the prior.\n",
      "    </li>\n",
      "</ol>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# repeat the above exercise with different bounds on the Amplitude vector"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    }
   ],
   "metadata": {}
  }
 ]
}